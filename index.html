<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>MLLM Tutorial</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/icon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Montserrat:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/bulma.min.css" rel="stylesheet">
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Bootslander
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/bootslander-free-bootstrap-landing-page-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top d-flex align-items-center header-transparent">
    <div class="container d-flex align-items-center justify-content-between">

      <div class="logo">
        <h1><a href="index.html"><span>MLLM Tutorial @ LREC-COLING 2024</span></a></h1>
        <!-- Uncomment below if you prefer to use an image logo -->
        <!-- <a href="index.html"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>-->
      </div>

      <nav id="navbar" class="navbar" style="background-color: #11af78;">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="#about">About</a></li>
<!--          <li><a class="nav-link scrollto" href="#features">Features</a></li>-->
<!--          <li><a class="nav-link scrollto" href="#gallery">Gallery</a></li>-->
          <li><a class="nav-link scrollto" href="#organizer">Organizer</a></li>
          <li><a class="nav-link scrollto" href="#schedule">Schedule</a></li>
          <li><a class="nav-link scrollto" href="#literature">Reading List</a></li>
          <li><a class="nav-link scrollto" href="#contact">Contact</a></li>
          <li class="dropdown"><a href="#"><span>Tutorial Series</span> <i class="bi bi-chevron-down"></i></a>
            <ul>
              <li><a href="https://mllm2024.github.io/CVPR2024/">CVPR 2024 Tutorial</a></li>
<!--              <li class="dropdown"><a href="#"><span>Deep Drop Down</span> <i class="bi bi-chevron-right"></i></a>-->
<!--                <ul>-->
<!--                  <li><a href="#">Deep Drop Down 1</a></li>-->
<!--                  <li><a href="#">Deep Drop Down 2</a></li>-->
<!--                  <li><a href="#">Deep Drop Down 3</a></li>-->
<!--                  <li><a href="#">Deep Drop Down 4</a></li>-->
<!--                  <li><a href="#">Deep Drop Down 5</a></li>-->
<!--                </ul>-->
<!--              </li>-->
<!--              <li><a href="#">Drop Down 2</a></li>-->
<!--              <li><a href="#">Drop Down 3</a></li>-->
<!--              <li><a href="#">Drop Down 4</a></li>-->
            </ul>
          </li>
<!--          <li><a class="nav-link scrollto" href="#contact">Contact</a></li>-->
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero">

    <div class="container">
      <div class="">
        <div class="col-lg-7 pt-5 pt-lg-0 order-2 order-lg-1 d-flex align-items-center" style="min-width: 100%;">
          <div data-aos="zoom-out" class="aos-init aos-animate">
            <h1>From <span>Multimodal LLM</span> to Human-level AI: </h1>
            <h2>Modality, Instruction, Reasoning, Efficiency and Beyond</h2>

          </div>
        </div>

      </div>
    </div>

    <svg class="hero-waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28 " preserveAspectRatio="none">
      <defs>
        <path id="wave-path" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z">
      </path></defs>
      <g class="wave1">
        <use xlink:href="#wave-path" x="50" y="3" fill="rgba(255,255,255, .1)">
      </use></g>
      <g class="wave2">
        <use xlink:href="#wave-path" x="50" y="0" fill="rgba(255,255,255, .2)">
      </use></g>
      <g class="wave3">
        <use xlink:href="#wave-path" x="50" y="9" fill="#fff">
      </use></g>
    </svg>

  </section><!-- End Hero -->

  <main id="main">


    <!-- ======= Features Section ======= -->
    <section id="about" class="about">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>About</h2>
          <p>MLLM Tutorial</p>
        </div>

          <div style="width=95%">
            <p>Welcome to the <b>MLLM Tutorial</b> series on <b><a class="green-text" href="https://lrec-coling-2024.org/">LREC-COLING 2024</a></b>!</p>
            <p  style="text-align:justify">
            Artificial intelligence (AI) encompasses knowledge acquisition and real-world grounding across various modalities.
              As a multidisciplinary research field, multimodal large language models (MLLMs) have recently garnered growing interest in both academia and industry,
              showing an unprecedented trend to achieve human-level AI via MLLMs.
              These large models offer an effective vehicle for understanding, reasoning, and planning by integrating and modeling diverse information modalities,
              including language, visual, auditory, and sensory data.
              This tutorial aims to deliver a comprehensive review of cutting-edge research in MLLMs, focusing on four key areas:
              MLLM architecture design, instructional learning, multimodal reasoning, and the efficiency of MLLMs.
              We will explore technical advancements, synthesize key challenges, and discuss potential avenues for future research.
            </p>

        </div>


      </div>
    </section>
    <!-- End Features Section -->


    <!-- ======= Team Section ======= -->
    <section id="organizer" class="team">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>Organizer</h2>
          <p>Presenters</p>
        </div>

        <div class="row" data-aos="fade-left" style="display: flex;flex-direction: row;align-items: center;justify-content: space-around;">

          <div class="col-lg-3 col-md-6" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="100">
              <div class="pic"><img src="assets/img/team/feihao.jpg" class="img-fluid" alt="https://haofei.vip/"></div>
              <div class="member-info">
                <h4><a href="https://haofei.vip">Hao Fei</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">National University of Singapore</span>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 mt-5 mt-md-0" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="200">
              <div class="pic"><img src="assets/img/team/yuanyao.jpg" class="img-fluid" alt=""></div>
              <div class="member-info">
                <h4><a href="https://yaoyuanthu.github.io/">Yuan Yao</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">National University of Singapore</span>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 mt-5 mt-lg-0" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="300">
              <div class="pic"><img src="assets/img/team/zzs.jpeg" class="img-fluid" alt=""></div>
              <div class="member-info">
                <h4><a href="https://bcmi.sjtu.edu.cn/~zhangzs/">Zhuosheng Zhang</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">Shanghai Jiao Tong University</span>
              </div>
            </div>
          </div>

        </div>
        <br>


        <div class="row" data-aos="fade-left" style="display: flex;flex-direction: row;align-items: center;justify-content: space-around;">

          <div class="col-lg-3 col-md-6" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="100">
              <div class="pic"><img src="assets/img/team/fxl.jpg" class="img-fluid" alt="https://haofei.vip/"></div>
              <div class="member-info">
                <h4><a href="https://fuxiaoliu.github.io">Fuxiao Liu</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">University of Maryland, College Park</span>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 mt-5 mt-md-0" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="200">
              <div class="pic"><img src="assets/img/team/ao.jpg" class="img-fluid" alt=""></div>
              <div class="member-info">
                <h4><a href="https://waxnkw.github.io">Ao Zhang</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">National University of Singapore</span>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 mt-5 mt-lg-0" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="300">
              <div class="pic"><img src="assets/img/team/chuats.jpg" class="img-fluid" alt=""></div>
              <div class="member-info">
                <h4><a href="https://www.chuatatseng.com/">Tat-seng Chua</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">National University of Singapore</span>
              </div>
            </div>
          </div>


        </div>




      </div>
    </section>
    <!-- End Team Section -->


    <!-- ======= Details Section ======= -->
<!--    <section id="details" class="details">-->
<!--      <div class="container">-->

<!--        <div class="row content">-->
<!--          <div class="col-md-4" data-aos="fade-right">-->
<!--            <img src="assets/img/details-1.png" class="img-fluid" alt="">-->
<!--          </div>-->
<!--          <div class="col-md-8 pt-4" data-aos="fade-up">-->
<!--            <h3>Voluptatem dignissimos provident quasi corporis voluptates sit assumenda.</h3>-->
<!--            <p class="fst-italic">-->
<!--              Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore-->
<!--              magna aliqua.-->
<!--            </p>-->
<!--            <ul>-->
<!--              <li><i class="bi bi-check"></i> Ullamco laboris nisi ut aliquip ex ea commodo consequat.</li>-->
<!--              <li><i class="bi bi-check"></i> Duis aute irure dolor in reprehenderit in voluptate velit.</li>-->
<!--              <li><i class="bi bi-check"></i> Iure at voluptas aspernatur dignissimos doloribus repudiandae.</li>-->
<!--              <li><i class="bi bi-check"></i> Est ipsa assumenda id facilis nesciunt placeat sed doloribus praesentium.</li>-->
<!--            </ul>-->
<!--            <p>-->
<!--              Voluptas nisi in quia excepturi nihil voluptas nam et ut. Expedita omnis eum consequatur non. Sed in asperiores aut repellendus. Error quisquam ab maiores. Quibusdam sit in officia-->
<!--            </p>-->
<!--          </div>-->
<!--        </div>-->

<!--        <div class="row content">-->
<!--          <div class="col-md-4 order-1 order-md-2" data-aos="fade-left">-->
<!--            <img src="assets/img/details-2.png" class="img-fluid" alt="">-->
<!--          </div>-->
<!--          <div class="col-md-8 pt-5 order-2 order-md-1" data-aos="fade-up">-->
<!--            <h3>Corporis temporibus maiores provident</h3>-->
<!--            <p class="fst-italic">-->
<!--              Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore-->
<!--              magna aliqua.-->
<!--            </p>-->
<!--            <p>-->
<!--              Ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate-->
<!--              velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in-->
<!--              culpa qui officia deserunt mollit anim id est laborum-->
<!--            </p>-->
<!--            <p>-->
<!--              Inventore id enim dolor dicta qui et magni molestiae. Mollitia optio officia illum ut cupiditate eos autem. Soluta dolorum repellendus repellat amet autem rerum illum in. Quibusdam occaecati est nisi esse. Saepe aut dignissimos distinctio id enim.-->
<!--            </p>-->
<!--          </div>-->
<!--        </div>-->

<!--        <div class="row content">-->
<!--          <div class="col-md-4" data-aos="fade-right">-->
<!--            <img src="assets/img/details-3.png" class="img-fluid" alt="">-->
<!--          </div>-->
<!--          <div class="col-md-8 pt-5" data-aos="fade-up">-->
<!--            <h3>Sunt consequatur ad ut est nulla consectetur reiciendis animi voluptas</h3>-->
<!--            <p>Cupiditate placeat cupiditate placeat est ipsam culpa. Delectus quia minima quod. Sunt saepe odit aut quia voluptatem hic voluptas dolor doloremque.</p>-->
<!--            <ul>-->
<!--              <li><i class="bi bi-check"></i> Ullamco laboris nisi ut aliquip ex ea commodo consequat.</li>-->
<!--              <li><i class="bi bi-check"></i> Duis aute irure dolor in reprehenderit in voluptate velit.</li>-->
<!--              <li><i class="bi bi-check"></i> Facilis ut et voluptatem aperiam. Autem soluta ad fugiat.</li>-->
<!--            </ul>-->
<!--            <p>-->
<!--              Qui consequatur temporibus. Enim et corporis sit sunt harum praesentium suscipit ut voluptatem. Et nihil magni debitis consequatur est.-->
<!--            </p>-->
<!--            <p>-->
<!--              Suscipit enim et. Ut optio esse quidem quam reiciendis esse odit excepturi. Vel dolores rerum soluta explicabo vel fugiat eum non.-->
<!--            </p>-->
<!--          </div>-->
<!--        </div>-->

<!--        <div class="row content">-->
<!--          <div class="col-md-4 order-1 order-md-2" data-aos="fade-left">-->
<!--            <img src="assets/img/details-4.png" class="img-fluid" alt="">-->
<!--          </div>-->
<!--          <div class="col-md-8 pt-5 order-2 order-md-1" data-aos="fade-up">-->
<!--            <h3>Quas et necessitatibus eaque impedit ipsum animi consequatur incidunt in</h3>-->
<!--            <p class="fst-italic">-->
<!--              Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore-->
<!--              magna aliqua.-->
<!--            </p>-->
<!--            <p>-->
<!--              Ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate-->
<!--              velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in-->
<!--              culpa qui officia deserunt mollit anim id est laborum-->
<!--            </p>-->
<!--            <ul>-->
<!--              <li><i class="bi bi-check"></i> Et praesentium laboriosam architecto nam .</li>-->
<!--              <li><i class="bi bi-check"></i> Eius et voluptate. Enim earum tempore aliquid. Nobis et sunt consequatur. Aut repellat in numquam velit quo dignissimos et.</li>-->
<!--              <li><i class="bi bi-check"></i> Facilis ut et voluptatem aperiam. Autem soluta ad fugiat.</li>-->
<!--            </ul>-->
<!--          </div>-->
<!--        </div>-->

<!--      </div>-->
<!--    </section>

&lt;!&ndash; End Details Section &ndash;&gt;-->


    <!-- ======= Gallery Section ======= -->
<!--    <section id="gallery" class="gallery">-->
<!--      <div class="container">-->

<!--        <div class="section-title" data-aos="fade-up">-->
<!--          <h2>Gallery</h2>-->
<!--          <p>Check our Gallery</p>-->
<!--        </div>-->

<!--        <div class="row g-0" data-aos="fade-left">-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="100">-->
<!--              <a href="assets/img/gallery/gallery-1.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-1.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="150">-->
<!--              <a href="assets/img/gallery/gallery-2.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-2.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="200">-->
<!--              <a href="assets/img/gallery/gallery-3.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-3.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="250">-->
<!--              <a href="assets/img/gallery/gallery-4.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-4.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="300">-->
<!--              <a href="assets/img/gallery/gallery-5.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-5.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="350">-->
<!--              <a href="#" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-6.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="400">-->
<!--              <a href="assets/img/gallery/gallery-7.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-7.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="450">-->
<!--              <a href="assets/img/gallery/gallery-8.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-8.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--        </div>-->

<!--      </div>-->
<!--    </section>-->
    <!-- End Gallery Section -->

    <!-- ======= Testimonials Section ======= -->
<!--    <section id="testimonials" class="testimonials">-->
<!--      <div class="container">-->

<!--        <div class="testimonials-slider swiper" data-aos="fade-up" data-aos-delay="100">-->
<!--          <div class="swiper-wrapper">-->

<!--            <div class="swiper-slide">-->
<!--              <div class="testimonial-item">-->
<!--                <img src="assets/img/testimonials/testimonials-1.jpg" class="testimonial-img" alt="">-->
<!--                <h3>Saul Goodman</h3>-->
<!--                <h4>Ceo &amp; Founder</h4>-->
<!--                <p>-->
<!--                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>-->
<!--                  Proin iaculis purus consequat sem cure digni ssim donec porttitora entum suscipit rhoncus. Accusantium quam, ultricies eget id, aliquam eget nibh et. Maecen aliquam, risus at semper.-->
<!--                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>-->
<!--                </p>-->
<!--              </div>-->
<!--            </div>&lt;!&ndash; End testimonial item &ndash;&gt;-->

<!--            <div class="swiper-slide">-->
<!--              <div class="testimonial-item">-->
<!--                <img src="assets/img/testimonials/testimonials-2.jpg" class="testimonial-img" alt="">-->
<!--                <h3>Sara Wilsson</h3>-->
<!--                <h4>Designer</h4>-->
<!--                <p>-->
<!--                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>-->
<!--                  Export tempor illum tamen malis malis eram quae irure esse labore quem cillum quid cillum eram malis quorum velit fore eram velit sunt aliqua noster fugiat irure amet legam anim culpa.-->
<!--                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>-->
<!--                </p>-->
<!--              </div>-->
<!--            </div>&lt;!&ndash; End testimonial item &ndash;&gt;-->

<!--            <div class="swiper-slide">-->
<!--              <div class="testimonial-item">-->
<!--                <img src="assets/img/testimonials/testimonials-3.jpg" class="testimonial-img" alt="">-->
<!--                <h3>Jena Karlis</h3>-->
<!--                <h4>Store Owner</h4>-->
<!--                <p>-->
<!--                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>-->
<!--                  Enim nisi quem export duis labore cillum quae magna enim sint quorum nulla quem veniam duis minim tempor labore quem eram duis noster aute amet eram fore quis sint minim.-->
<!--                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>-->
<!--                </p>-->
<!--              </div>-->
<!--            </div>&lt;!&ndash; End testimonial item &ndash;&gt;-->

<!--            <div class="swiper-slide">-->
<!--              <div class="testimonial-item">-->
<!--                <img src="assets/img/testimonials/testimonials-4.jpg" class="testimonial-img" alt="">-->
<!--                <h3>Matt Brandon</h3>-->
<!--                <h4>Freelancer</h4>-->
<!--                <p>-->
<!--                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>-->
<!--                  Fugiat enim eram quae cillum dolore dolor amet nulla culpa multos export minim fugiat minim velit minim dolor enim duis veniam ipsum anim magna sunt elit fore quem dolore labore illum veniam.-->
<!--                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>-->
<!--                </p>-->
<!--              </div>-->
<!--            </div>&lt;!&ndash; End testimonial item &ndash;&gt;-->

<!--            <div class="swiper-slide">-->
<!--              <div class="testimonial-item">-->
<!--                <img src="assets/img/testimonials/testimonials-5.jpg" class="testimonial-img" alt="">-->
<!--                <h3>John Larson</h3>-->
<!--                <h4>Entrepreneur</h4>-->
<!--                <p>-->
<!--                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>-->
<!--                  Quis quorum aliqua sint quem legam fore sunt eram irure aliqua veniam tempor noster veniam enim culpa labore duis sunt culpa nulla illum cillum fugiat legam esse veniam culpa fore nisi cillum quid.-->
<!--                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>-->
<!--                </p>-->
<!--              </div>-->
<!--            </div>&lt;!&ndash; End testimonial item &ndash;&gt;-->

<!--          </div>-->
<!--          <div class="swiper-pagination"></div>-->
<!--        </div>-->

<!--      </div>-->
<!--    </section>-->
    <!-- End Testimonials Section -->



 <!-- ======= Schedule Section ======= -->
    <section id="schedule" class="schedule">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>Schedule</h2>
          <p>PROGRAM</p>
        </div>

  <div class="container">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <p>
          Our tutorial will be held on Tuesday, May 21, 2024 (all the times are based on UTC + 2 = Torino local time). 
        </p>

	<br>

        <p>
          Our tutorial online video record (only Part 1-3, due to technical issue) is posted on Youtube, <a href="https://www.youtube.com/watch?v=-oKpZjaKsAQ">visit here to watch</a>. 
        </p>

	<br>
        <div class="content has-text-justified">

          <style type="text/css">
          .tg  {border-collapse:collapse;border-spacing:0;}
          .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
          .tg .tg-0lax{text-align:left;vertical-align:top}
          </style>
          <table class="tg" style="font-weight: bold;">
          <thead>
            <tr>
              <th class="tg-0pky" style="font-weight: bold;">Time</th>
              <th class="tg-0lax" style="font-weight: bold;">Section</th>
              <th class="tg-0lax" style="font-weight: bold;">Presenter</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-0lax">14:00-14:10</td>
              <td class="tg-0lax">Part 1: Background and Introduction  <a href="assets/slides/MLLM-part1-Hao.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Hao Fei</td>
            </tr>
            <tr>
              <td class="tg-0lax">14:10-15:40</td>
              <td class="tg-0lax">Part 2: MLLM Design: Architecture and Modality <a href="assets/slides/MLLM-part2-Hao&Yuan.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Hao Fei & Yuan Yao</td>
            </tr>
            <tr>
              <td class="tg-0lax">15:40-16:00</td>
              <td class="tg-0lax">Part 3: Multimodal instruction Tuning in MLLMs <a href="assets/slides/MLLM-part3-Fuxiao.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Fuxiao Liu</td>
            </tr>
            <tr>
              <td class="tg-0lax"></td>
              <td class="tg-0lax">Coffee Break, Q&A Session</a></td>
              <td class="tg-0lax"></td>
            </tr>
            <tr>
              <td class="tg-0lax">16:30-16:50</td>
              <td class="tg-0lax">Part 3 (Cont'd): Multimodal instruction Tuning in MLLMs</td>
              <td class="tg-0lax">Fuxiao Liu</td>
            </tr>
            <tr>
              <td class="tg-0lax">16:50-17:30</td>
              <td class="tg-0lax">Part 4: Multimodal Reasoning in MLLMs  <a href="assets/slides/MLLM-part4-Zhuosheng.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Zhuosheng Zhang</td>
            </tr>
            <tr>
              <td class="tg-0lax">17:30-18:00</td>
              <td class="tg-0lax">Part 5: MLLM Efficiency <a href="assets/slides/MLLM-part5-Ao.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Ao Zhang</td>
            </tr>
          </tbody>
          </table>
        </div>
      </div>
    </div>



<!--        <p>Our tutorial will be held on July 9 (all the times are based on EDT = Toronto local time).-->
<!--        </p>-->
<!--        <p><em>Slides may be subject to updates.</em></p>-->


<!--        <div class="row" data-aos="fade-left">-->

<!--		<p>Date: <b>November 2, 2023 (full day)</b>. Room: <b>Provinces 1</b>.-->
<!--		Please note the schedule is in Ottawa time zone. The program at a glance can be downloaded <a href="https://www.acmmm2023.org/wp-content/uploads/2023/10/10_04_ACM-MM-2023-Program_at_glance.pdf">here</a>.-->
<!--		</p>-->

<!--		<p>Also you can online join via <a href="https://nus-sg.zoom.us/j/3358253206?pwd=Ni9YMjNqRCtZYm42aHdIVU9wSG5hZz09">Zoom Meeting (click to enter)</a>, ID: 335 825 3206, Passcode: 118404</p>-->
<!--		<br>-->

<!--                <div class="container-table100">-->
<!--                    <div class="wrap-table100">-->

<!--                        <div class="table100 ver5 m-b-10">-->
<!--                            <table data-vertable="ver5">-->
<!--                                <tbody>-->

<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">09:00 - 09:10</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b>Welcome Message from the Chairs</b></td>-->
<!--                                </tr>-->


<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">09:10 - 09:40</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b class="orange-text">Keynote 1: Hypergraphs for Multimedia Retrieval and Insight</b>, by Prof. Marcel Worring</td>-->
<!--                                </tr>-->



<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">09:40 - 10:10</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b class="orange-text">Keynote 2: Revisiting Pseudo Relevance Feedback: New Developments and Applications</b>, by Prof. Shin’ichi Satoh</td>-->
<!--                                </tr>-->


<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">10:10 - 10:40</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b>Coffee Break</b></td>-->
<!--                                </tr>-->




<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">10:40 - 11:00</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b style="color: #2985fc;">Presentation 1: Video Referring Expression Comprehension via Transformer with Content-conditioned Query</b></td>-->
<!--                                </tr>-->


<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">11:00 - 11:20</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b style="color: #2985fc;">Presentation 2: Prescription Recommendation based on Intention Retrieval Network and Multimodal Medical Indicator</b></td>-->
<!--                                </tr>-->


<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">11:20 - 11:40</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b style="color: #2985fc;">Presentation 3: Self-Distilled Dynamic Network for Language-based Fashion Retrieval</b></td>-->
<!--                                </tr>-->

<!--				<tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">11:40 - 12:00</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b style="color: #2985fc;">Presentation 4: Metaverse Retrieval: Finding the Best Metaverse Environment via Language</b></td>-->
<!--                                </tr>-->

<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">12:00 - 12:30</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b class="orange-text">Keynote 3: Task Focused IR in the Era of Generative AI</b>, by Prof. Chirag Shah</td>-->
<!--                                </tr>-->


<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">12:30 - 15:00</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b>Lunch Break</b></td>-->
<!--                                </tr>-->





<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">15:00 - 15:20</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b style="color: #2985fc;">Presentation 5: On Popularity Bias of Multimodal-aware Recommender Systems: a Modalities-driven Analysis</b></td>-->
<!--                                </tr>-->


<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">15:20 - 15:40</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b style="color: #2985fc;">Presentation 6: Boon: A Neural Search Engine for Cross-Modal Information Retrieval</b></td>-->
<!--                                </tr>-->





<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">15:40 - 16:10</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b>Coffee Break</b></td>-->
<!--                                </tr>-->



<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">16:10 - 16:30</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b style="color: #2985fc;">Presentation 7: TC-OCR: TableCraft OCR for Efficient Detection & Recognition of Table Structure & Content</b></td>-->
<!--                                </tr>-->


<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">16:30 - 16:50</td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b style="color: #2985fc;">Presentation 8: Zero-Shot Composed Image Retrieval with Textual Inversion</b></td>-->
<!--                                </tr>-->




<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">16:50 </td>-->
<!--                                    <td class="column100 column1" data-column="column1">&nbsp; | &nbsp;</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><b>Workshop Closing</b></td>-->
<!--                                </tr>-->




<!--                                </tbody>-->

<!--                            </table>-->
<!--                        </div>-->
<!--                    </div>-->
<!--                </div>-->
















<!--                <table data-vertable="ver5">-->
<!--                                <thead>-->
<!--                                <tr class="row100 head">-->
<!--                                    <th class="column100 column1" data-column="column1"><strong>Time</strong></th>-->
<!--                                    <th class="column100 column2" data-column="column2"><strong>Event</strong></th>-->
<!--                                </tr>-->
<!--                                </thead>-->
<!--                                <tbody>-->

<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">08:00 - 08:15</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><strong>Welcome and Opening-->
<!--                                        Remarks</strong></td>-->
<!--                                </tr>-->

<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">08:15 - 09:15</td>-->
<!--                                    <td class="column100 column2" data-column="column2">-->
<!--                                        <a style="color: #ffc107;">-->
<!--                                            <strong>Paper session 1</strong>-->
<!--                                        </a>-->
<!--                                    </td>-->
<!--                                </tr>-->


<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">09:30 - 10:30</td>-->
<!--                                    <td class="column100 column2" data-column="column2">-->
<!--                                        <a style="color: #000000;">-->
<!--                                            <strong>Keynote 1 (20 mins) and Keynote 2 (20 mins), with joint Q&amp;A (20 mins)</strong>-->
<!--                                        </a>-->
<!--                                        <br> <em>speaker:</em> &nbsp; <font color="red">Preslav and Farah</font> &nbsp;  &nbsp;-->
<!--                                    </td>-->
<!--                                </tr>-->

<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">10:30 - 11:30</td>-->
<!--                                    <td class="column100 column2" data-column="column2">-->
<!--                                        <a style="color: #ffc107;">-->
<!--                                            <strong>Paper session 2</strong>-->
<!--                                        </a>-->
<!--                                    </td>-->
<!--                                </tr>-->

<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">11:30 - 12:30</td>-->
<!--                                    <td class="column100 column2" data-column="column2">-->
<!--                                        <a style="color: #000000;">-->
<!--                                            <strong>Keynote 3 (20 mins) and Keynote 4 (20 mins), with joint Q&amp;A (20 mins)</strong>-->
<!--                                        </a>-->
<!--                                        <br> <em>speaker:</em> &nbsp; <font color="red">Vivian and Tanya</font> &nbsp; &nbsp;-->
<!--                                    </td>-->
<!--                                </tr>-->

<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">12:30 - 13:00</td>-->
<!--                                    <td class="column100 column2" data-column="column2">-->
<!--					    <a style="color: #2bc94f;">-->
<!--						    <strong>Poster lightning talks</strong></a></td>-->


<!--                                </tr>-->

<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">13:00 - 14:00</td>-->
<!--                                    <td class="column100 column2" data-column="column2">-->
<!--                                            <a style="color: #007bff;"><strong>Poster session 1 and Lunch</strong> </a>-->
<!--                                    </td>-->
<!--                                </tr>-->

<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">14:00 - 15:00</td>-->
<!--                                    <td class="column100 column2" data-column="column2">-->
<!--                                        <a style="color: #ffc107;">-->
<!--                                            <strong>Paper session 3</strong>-->
<!--                                        </a>-->
<!--                                    </td>-->
<!--                                </tr>-->


<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">15:00 - 16:00</td>-->
<!--                                    <td class="column100 column2" data-column="column2">-->
<!--                                        <a style="color: #000000;">-->
<!--                                            <strong>Keynote 5 (20 mins) and Keynote 6 (20 mins), with joint Q&amp;A (20 mins)</strong>-->
<!--                                        </a>-->
<!--                                        <br> <em>speaker:</em> &nbsp; <font color="red">Diyi and Joao</font>-->
<!--				     </td>-->
<!--                                </tr>-->

<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">16:00 - 16:30</td>-->
<!--                                    <td class="column100 column2" data-column="column2">-->

<!--                                        <a style="color: #007bff;">-->
<!--                                            <strong>Poster session 2 and Tea</strong>-->
<!--                                        </a>-->
<!--				     </td>-->
<!--                                </tr>-->

<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">16:30 - 18:00</td>-->
<!--                                    <td class="column100 column2" data-column="column2">-->

<!--                                        <a style="color: #000000;">-->
<!--                                            <strong>Industry session: Speaker presentations &amp; Q&amp;A</strong>-->
<!--                                        </a>-->
<!--                                        <br> <em>speaker:</em> &nbsp; <font color="red">Daniel, Huda, Alessandro, Lidong</font>-->
<!--				     </td>-->
<!--                                </tr>-->

<!--                                <tr class="row100">-->
<!--                                    <td class="column100 column1" data-column="column1">18:00 - 18:30</td>-->
<!--                                    <td class="column100 column2" data-column="column2"><strong>Townhall</strong>-->
<!--                                    </td>-->
<!--                                </tr>-->

<!--                                </tbody>-->

<!--                            </table>-->



<!--        <table class="tg">-->
<!--          <thead>-->
<!--            <tr>-->
<!--              <th class="tg-0pky">Time</th>-->
<!--              <th class="tg-0lax">Section</th>-->
<!--              <th class="tg-0lax">Presenter</th>-->
<!--            </tr>-->
<!--          </thead>-->
<!--          <tbody>-->
<!--            <tr>-->
<!--              <td class="tg-0lax">14:00—14:15</td>-->
<!--              <td class="tg-0lax">Section 1: Introduction  <a href="./assets/slides/1-intro.pdf" target="_blank">[Slides]</a></td>-->
<!--              <td class="tg-0lax">Danqi</td>-->
<!--            </tr>-->
<!--            <tr>-->
<!--              <td class="tg-0lax">14:15—14:25</td>-->
<!--              <td class="tg-0lax">Section 2: Definition &amp; Preliminaries  <a href="./assets/slides/2-definition.pdf" target="_blank">[Slides]</a></td>-->
<!--              <td class="tg-0lax">Sewon</td>-->
<!--            </tr>-->
<!--            <tr>-->
<!--              <td class="tg-0lax">14:25—15:00</td>-->
<!--              <td class="tg-0lax">Section 3: Retrieval-based LMs: Architecture  <a href="./assets/slides/3-architecture.pdf" target="_blank">[Slides]</a></td>-->
<!--              <td class="tg-0lax">Sewon</td>-->
<!--            </tr>-->
<!--            <tr>-->
<!--              <td class="tg-0lax">15:00—15:25</td>-->
<!--              <td class="tg-0lax">Section 4: Retrieval-based LMs: Training  <a href="./assets/slides/4-training.pdf" target="_blank">[Slides]</a></td>-->
<!--              <td class="tg-0lax">Zexuan</td>-->
<!--            </tr>-->
<!--            <tr>-->
<!--              <td class="tg-0lax">15:25—15:30</td>-->
<!--              <td class="tg-0lax">Q &amp; A Session I</td>-->
<!--              <td class="tg-0lax"></td>-->
<!--            </tr>-->
<!--            <tr>-->
<!--              <td class="tg-0lax">15:30—16:00</td>-->
<!--              <td class="tg-0lax">Coffee break</td>-->
<!--              <td class="tg-0lax"></td>-->
<!--            </tr>-->
<!--            <tr>-->
<!--              <td class="tg-0lax">16:00—16:25</td>-->
<!--              <td class="tg-0lax">Section 4 (Cont’d): Retrieval-based LMs: Training  <a href="./assets/slides/4-training.pdf" target="_blank">[Slides]</a></td>-->
<!--              <td class="tg-0lax">Zexuan</td>-->
<!--            </tr>-->
<!--            <tr>-->
<!--              <td class="tg-0lax">16:25—17:00</td>-->
<!--              <td class="tg-0lax">Section 5: Retrieval-based LMs: Applications  <a href="./assets/slides/5-application.pdf" target="_blank">[Slides]</a></td>-->
<!--              <td class="tg-0lax">Akari</td>-->
<!--            </tr>-->
<!--            <tr>-->
<!--              <td class="tg-0lax">17:00—17:10</td>-->
<!--              <td class="tg-0lax">Section 6: Extension: Multilingual &amp; Multimodal  <a href="./assets/slides/6-extension.pdf" target="_blank">[Slides]</a></td>-->
<!--              <td class="tg-0lax">Akari</td>-->
<!--            </tr>-->
<!--            <tr>-->
<!--              <td class="tg-0lax">17:10—17:20</td>-->
<!--              <td class="tg-0lax">Section 7: Challenges &amp; Opportunities  <a href="./assets/slides/7-conclusion.pdf" target="_blank">[Slides]</a> <a href="./slides/references.pdf" target="_blank">[References]</a></td>-->
<!--              <td class="tg-0lax">Danqi</td>-->
<!--            </tr>-->
<!--            <tr>-->
<!--              <td class="tg-0lax">17:20—17:30</td>-->
<!--              <td class="tg-0lax">Q &amp; A Session II</td>-->
<!--              <td class="tg-0lax"></td>-->
<!--            </tr>-->
<!--          </tbody>-->
<!--          </table>-->



        </div>

      </div>
    </section>
    <!-- End Schedule Section -->




    <!-- ======= Literature Section ======= -->
    <section id="literature" class="literature">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>Literature</h2>
          <p>Reading List</p>
        </div>

<!--          <p>Frequently Asked Questions</p>-->
        <div class="faq-list">

          <h4 class="title is-2">Section 1: LLMs and MLLMs</h4>
          <div>
            <ol>
              <li>
                OpenAI, 2023, <a href="https://openai.com/blog/chatgpt"><b>Introducing ChatGPT</b></a><br>
              </li>
              <li>
                OpenAI, 2023, <a href="https://arxiv.org/abs/2303.08774"><b>GPT-4 Technical Report</b></a><br>
              </li>
              <li>
                Alayrac, et al., 2022, <a href="https://arxiv.org/abs/2204.14198"><b>Flamingo: a Visual Language Model for Few-Shot Learning</b></a><br>
              </li>
              <li>
                Li, et al., 2023, <a href="https://arxiv.org/abs/2301.12597"><b>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</b></a><br>
              </li>		    
              <li>
                Zhu, et al., 2023, <a href="https://arxiv.org/abs/2304.10592"><b>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</b></a><br>
              </li>
              <li>
                Wu, et al., 2023, <a href="https://arxiv.org/abs/2303.04671"><b>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</b></a><br>
              </li>		    
              <li>
                Shen, et al., 2023, <a href="https://arxiv.org/abs/2303.17580"><b>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</b></a><br>
              </li>		    
              <li>
                Tang, et al., 2023, <a href="https://arxiv.org/abs/2305.11846"><b>Any-to-Any Generation via Composable Diffusion</b></a><br>
              </li>
              <li>
                Girdhar, et al., 2023, <a href="https://arxiv.org/abs/2305.05665"><b>ImageBind: One Embedding Space To Bind Them All</b></a><br>
              </li>		    
              <li>
                Wu, et al., 2023, <a href="https://arxiv.org/abs/2309.05519"><b>NExT-GPT: Any-to-Any Multimodal LLM</b></a><br>
              </li>		    
              <li>
                Moon, et al., 2023, <a href="https://arxiv.org/abs/2309.16058"><b>AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model</b></a><br>
              </li>

		    
              <li>
                Hu, et al., 2023, <a href="https://arxiv.org/abs/2308.12038"><b>Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages</b></a><br>
              </li>
		    
              <li>
                Bai, et al., 2023, <a href="https://arxiv.org/abs/2308.12966"><b>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</b></a><br>
              </li>

              <li>
                Wang, et al., 2023, <a href="https://arxiv.org/abs/2311.03079"><b>CogVLM: Visual Expert for Pretrained Language Models</b></a><br>
              </li>
		    
              <li>
                Peng, et al., 2023, <a href="https://arxiv.org/abs/2306.14824"><b>Kosmos-2: Grounding Multimodal Large Language Models to the World</b></a><br>
              </li>

              <li>
                Dong, et al., 2023, <a href="https://arxiv.org/abs/2401.16420"><b>InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model</b></a><br>
              </li>
              
              <!-- modified start -->
              <li>
                Zhu, et al., 2023, <a href="https://arxiv.org/abs/2310.01852"><b>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment</b></a><br>
              </li>
              <li>
                Ge, et al., 2023, <a href="https://arxiv.org/abs/2307.08041"><b>Planting a SEED of Vision in Large Language Model</b></a><br>
              </li>
              <li>
                Zhan, et al., 2024, <a href="https://arxiv.org/abs/2402.12226"><b>AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</b></a><br>
              </li>
              <li>
                Kondratyuk, et al., 2023, <a href="https://arxiv.org/abs/2312.14125"><b>VideoPoet: A Large Language Model for Zero-Shot Video Generation</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2308.16692"><b>SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models</b></a><br>
              </li>
              <li>
                Zeghidour, et al., 2021, <a href="https://arxiv.org/abs/2107.03312"><b>SoundStream: An End-to-End Neural Audio Codec</b></a><br>
              </li>
              <li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2310.03744"><b>Improved Baselines with Visual Instruction Tuning</b></a><br>
              </li>
              <li>
                Wu, et al., 2023, <a href="https://arxiv.org/abs/2303.04671"><b>Visual-ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</b></a><br>
              </li>
              <li>
                Wang, et al., 2023, <a href="https://arxiv.org/abs/2401.06395"><b>ModaVerse: Efficiently Transforming Modalities with LLMs</b></a><br>
              </li>
              <li>
                Fei, et al., 2024, <a href="http://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf"><b>VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing</b></a><br>
              </li>
              <li>
                Lu, et al., 2023, <a href="https://arxiv.org/abs/2312.17172"><b>Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</b></a><br>
              </li>
              <li>
                Bai, et al., 2023, <a href="https://arxiv.org/abs/2312.00785"><b>LVM: Sequential Modeling Enables Scalable Learning for Large Vision Models</b></a><br>
              </li>
              <li>
                Huang, et al., 2023, <a href="https://arxiv.org/abs/2302.14045"><b>Language Is Not All You Need: Aligning Perception with Language Models</b></a><br>
              </li>
              <li>
                Li, et al., 2023, <a href="https://arxiv.org/abs/2305.06355"><b>VideoChat: Chat-Centric Video Understanding</b></a><br>
              </li>
              <li>
                Maaz, et al., 2023, <a href="https://arxiv.org/abs/2306.05424"><b>Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2306.02858"><b>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</b></a><br>
              </li>
              <li>
                Lin, et al., 2023, <a href="https://arxiv.org/abs/2311.10122"><b>Video-LLaVA: Learning United Visual Representation by Alignment Before Projection</b></a><br>
              </li>
              <li>
                Qian, et al., 2024, <a href="https://arxiv.org/abs/2402.11435"><b>Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning</b></a><br>
              </li>
              <li>
                Hong, et al., 2023, <a href="https://arxiv.org/abs/2307.12981"><b>3D-LLM: Injecting the 3D World into Large Language Models</b></a><br>
              </li>
              <li>
                Sun, et al., 2023, <a href="https://arxiv.org/abs/2310.12945"><b>3D-GPT: Procedural 3D Modeling with Large Language Models</b></a><br>
              </li>
              <li>
                Chen, et al., 2023, <a href="https://arxiv.org/abs/2311.18651"><b>LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning</b></a><br>
              </li>
              <li>
                Xu, et al., 2023, <a href="https://arxiv.org/abs/2308.16911"><b>PointLLM: Empowering Large Language Models to Understand Point Clouds</b></a><br>
              </li>
              <li>
                Chen, et al., 2024, <a href="https://arxiv.org/abs/2401.12168"><b>SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities</b></a><br>
              </li>
              <li>
                Huang, et al., 2023, <a href="https://arxiv.org/abs/2304.12995"><b>AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2305.11000"><b>SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities</b></a><br>
              </li>
              <li>
                Wang, et al., 2023, <a href="https://arxiv.org/abs/2305.16107"><b>VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation</b></a><br>
              </li>
              <li>
                Rubenstein, et al., 2023, <a href="https://arxiv.org/abs/2306.12925"><b>AudioPaLM: A Large Language Model That Can Speak and Listen</b></a><br>
              </li>
              <li>
                Tang, et al., 2023, <a href="https://arxiv.org/abs/2310.13289"><b>SALMONN: Towards Generic Hearing Abilities for Large Language Models</b></a><br>
              </li>
              <li>
                Latif, et al., 2023, <a href="https://arxiv.org/abs/2310.13289"><b>Sparks of Large Audio Models: A Survey and Outlook</b></a><br>
              </li>
              <li>
                Luo, et al., 2022, <a href="https://arxiv.org/abs/2210.10341"><b>BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining</b></a><br>
              </li>
              <li>
                Li, et al., 2023, <a href="https://doi.org/10.1101/2023.06.29.543848"><b>DrugGPT: A GPT-based Strategy for Designing Potential Ligands Targeting Specific Proteins</b></a><br>
              </li>
              <li>
                Chen, et al., 2023, <a href="https://arxiv.org/abs/2311.16079"><b>MEDITRON-70B: Scaling Medical Pretraining for Large Language Models</b></a><br>
              </li>
              <li>
                Wang, et al., 2023, <a href="https://arxiv.org/abs/2304.06975"><b>HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2310.14558"><b>AlpaCare:Instruction-tuned Large Language Models for Medical Application</b></a><br>
              </li>
              <li>
                Frey, et al., 2023, <a href="https://www.nature.com/articles/s42256-023-00740-3"><b>Neural Scaling of Deep Chemical Models</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2402.06852"><b>ChemLLM: A Chemical Large Language Model</b></a><br>
              </li>
              <li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2310.12798"><b>MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter</b></a><br>
              </li>
              <li>
                Jiang, et al., 2023, <a href="https://arxiv.org/abs/2305.09645"><b>StructGPT: A General Framework for Large Language Model to Reason on Structured Data</b></a><br>
              </li>
              <li>
                Chen, et al., 2024, <a href="https://arxiv.org/abs/2402.08170"><b>LLaGA: Large Language and Graph Assistant</b></a><br>
              </li>
              <li>
                Koh, et al., 2023, <a href="https://arxiv.org/abs/2305.17216"><b>Generating Images with Multimodal Language Models</b></a><br>
              </li>
              <li>
                Sun, et al., 2023, <a href="https://arxiv.org/abs/2307.05222"><b>Generative Pretraining in Multimodality</b></a><br>
              </li>
              <li>
                Zheng, et al., 2023, <a href="https://arxiv.org/abs/2310.02239"><b>MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens</b></a><br>
              </li>
              <li>
                Dong, et al., 2023, <a href="https://arxiv.org/abs/2309.11499"><b>DreamLLM: Synergistic Multimodal Comprehension and Creation</b></a><br>
              </li>
              <li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2311.05437"><b>LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents</b></a><br>
              </li>
              <li>
                Wang, et al., 2023, <a href="https://arxiv.org/abs/2311.16511"><b>GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation</b></a><br>
              </li>
              <li>
                Jin, et al., 2024, <a href="https://arxiv.org/abs/2402.03161"><b>Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization</b></a><br>
              </li>
              <li>
                Jin, et al., 2023, <a href="https://arxiv.org/abs/2311.08046"><b>Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding</b></a><br>
              </li>
              <li>
                Li, et al., 2023, <a href="https://arxiv.org/abs/2311.17043"><b>LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models</b></a><br>
              </li>
              <li>
                Su, et al., 2023, <a href="https://arxiv.org/abs/2305.16355"><b>PandaGPT: One Model to Instruction-Follow Them All</b></a><br>
              </li>
              <li>
                Lyu, et al., 2023, <a href="https://arxiv.org/abs/2306.09093"><b>Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration</b></a><br>
              </li>
              <li>
                Tang, et al., 2023, <a href="https://arxiv.org/abs/2311.18775"><b>CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2307.03601"><b>GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest</b></a><br>
              </li>
              <li>
                Yuan, et al., 2023, <a href="https://arxiv.org/abs/2312.10032"><b>Osprey: Pixel Understanding with Visual Instruction Tuning</b></a><br>
              </li>
              <li>
                Rasheed, et al., 2023, <a href="https://arxiv.org/abs/2311.03356"><b>GLaMM: Pixel Grounding Large Multimodal Model</b></a><br>
              </li>
              <li>
                Pi, et al., 2023, <a href="https://arxiv.org/abs/2305.14167"><b>DetGPT: Detect What You Need via Reasoning</b></a><br>
              </li>
              <li>
                Ren, et al., 2023, <a href="https://arxiv.org/abs/2312.02228"><b>PixelLM: Pixel Reasoning with Large Multimodal Model</b></a><br>
              </li>
              <li>
                Lai, et al., 2023, <a href="https://arxiv.org/abs/2308.00692"><b>Lisa: Reasoning segmentation via large language model</b></a><br>
              </li>
              <li>
                Chen, et al., 2023, <a href="https://arxiv.org/abs/2306.15195"><b>Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic</b></a><br>
              </li>
              <li>
                Munasinghe, et al., 2023, <a href="https://arxiv.org/abs/2311.13435"><b>PG-Video-LLaVA: Pixel Grounding in Large Multimodal Video Models</b></a><br>
              </li>
              <li>
                Yu, et al., 2023, <a href="https://arxiv.org/abs/2312.00589"><b>Merlin: Empowering Multimodal LLMs with Foresight Minds</b></a><br>
              </li>
              <li>
                Fu, et al., 2023, <a href="https://arxiv.org/abs/2306.13394"><b>MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</b></a><br>
              </li>
              <li>
                Xu, et al., 2023, <a href="https://arxiv.org/abs/2306.09265"><b>LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models</b></a><br>
              </li>
              <li>
                Ying, et al., 2024, <a href="https://arxiv.org/abs/2404.16006"><b>MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI</b></a><br>
              </li>
              <li>
                Pan, et al., 2024, <a href="https://arxiv.org/abs/2405.01926"><b>Auto-Encoding Morph-Tokens for Multimodal LLM</b></a><br>
              </li>
              <li>
                Thagard, et al., 1997, <a href="https://link.springer.com/chapter/10.1007/978-94-017-0487-8_22"><b>Abductive reasoning: Logic, visual thinking, and coherence</b></a><br>
              </li>
              <li>
                Bavishi, et al., 2023, <a href="https://www.adept.ai/blog/fuyu-8b"><b>Fuyu-8B: A Multimodal Architecture for AI Agents</b></a><br>
              </li>

              <!-- modified end -->
            </ol>
        </div>

        <br>
          <h4 class="title is-2">Section 2: Instruction Tuning</h4>
          <div>
            <ol>
              <li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2304.08485"><b>Visual Instruction Tuning</b></a><br>
              </li>
              <li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2306.14565"><b>Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning</b></a><br>
              </li>
              <li>
                Gao, et al., 2023, <a href="https://arxiv.org/abs/2304.15010"><b>LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model</b></a><br>
              </li>
              <li>
                Zhao, et al., 2023, <a href="https://arxiv.org/abs/2307.04087"><b>SVIT: Scaling up Visual Instruction Tuning</b></a><br>
              </li>
              <li>
                Ye, et al., 2023, <a href="https://arxiv.org/abs/2304.14178"><b>mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality</b></a><br>
              </li>
              <li>
                Yu, et al., 2023, <a href="https://arxiv.org/abs/2312.00849"><b>RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback</b></a><br>
              </li>
		<li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2311.10774"><b>MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning</b></a><br>
              </li>
	       <li>
                Zhao, et al., 2023, <a href="https://arxiv.org/abs/2304.10592"><b>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</b></a><br>
              </li>
	       <li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2310.14566"><b>HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models</b></a><br>
              </li>
	       <li>
                Li, et al., 2023, <a href="https://arxiv.org/abs/2305.10355"><b>Evaluating Object Hallucination in Large Vision-Language Models</b></a><br>
              </li>
	      <li>
                Huang, et al., 2023, <a href="https://arxiv.org/abs/2306.13549"><b>Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey</b></a><br>
              </li>
	      <li>
                Yin, et al., 2023, <a href="https://arxiv.org/abs/2312.16602"><b>A Survey on Multimodal Large Language Models</b></a><br>
              </li>
	      <li>
                Yin, et al., 2023, <a href="https://arxiv.org/abs/2312.16602"><b>Woodpecker: Hallucination Correction for Multimodal Large Language Models</b></a><br>
              </li>
            </ol>
        </div>



        <br>
          <h4 class="title is-2">Section 3: Reasoning with LLM</h4>
          <div>
            <ol>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2302.00923"><b>Multimodal Chain-of-Thought Reasoning in Language Models</b></a><br>
              </li>
              <li>
                Zhao, et al., 2023, <a href="https://arxiv.org/abs/2309.07915"><b>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning</b></a><br>
              </li>
              <li>
                Lu, et al., 2023, <a href="https://arxiv.org/abs/2304.09842"><b>Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2309.11436"><b>You Only Look at Screens: Multimodal Chain-of-Action Agents</b></a><br>
              </li>
              <li>
                Sun, et al., 2023, <a href="https://arxiv.org/abs/2312.13286"><b>Generative multimodal models are in-context learners</b></a><br>
              </li>
              <li>
                Fei, et al., 2023, <a href="https://vitron-llm.github.io/"><b>VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing</b></a><br>
              </li>
              <li>
                Wei, et al., 2023, <a href="https://arxiv.org/abs/2307.12626"><b>Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2311.11797"><b>Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents</b></a><br>
              </li>
               <li>
                Fei, et al., 2024, <a href="http://haofei.vip/VoT/"><b>Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition</b></a><br>
              </li>
              <li>
                Prystawski, et al., 2023, <a href="https://arxiv.org/abs/2304.03843"><b>Why think step by step? Reasoning emerges from the locality of experience</b></a><br>
              </li>
              <li>
                Gou, et al., 2023, <a href="https://arxiv.org/abs/2305.11738"><b>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</b></a><br>
              </li>
              <li>
                Tang, et al., 2024, <a href="https://arxiv.org/abs/2402.04247"><b>Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science</b></a><br>
              </li>
              <li>
                Yuan, et al., 2024, <a href="https://arxiv.org/abs/2401.10019"><b>R-Judge: Benchmarking Safety Risk Awareness for LLM Agents</b></a><br>
              </li>
            </ol>
        </div>


        <br>
          <h4 class="title is-2">Section 4: Efficient Learning</h4>
          <div>
            <ol>
              <li>
                Hu, et al., 2021, <a href="https://arxiv.org/abs/2106.09685"><b>LoRA: Low-Rank Adaptation of Large Language Models</b></a><br>
              </li>
              <li>
                Dettmers, et al., 2023, <a href="https://arxiv.org/abs/2305.14314"><b>QLoRA: Efficient Finetuning of Quantized LLMs</b></a><br>
              </li>
              <li>
                Li, et al., 2023, <a href="https://arxiv.org/abs/2301.12597"><b>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</b></a><br>
              </li>
              <li>
                Luo, et al., 2023, <a href="https://arxiv.org/abs/2305.15023"><b>Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models</b></a><br>
              </li>
              <li>
                Yao, et al., 2024, <a href="https://github.com/OpenBMB/MiniCPM-V"><b>MiniCPM-V</b></a><br>
              </li>
              <li>
                 DeepSpeed Team, 2020, <a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/"><b>DeepSpeed Blog</b></a><br>
              </li>
              <li>
                 Zhao, et al., 2023, <a href="https://arxiv.org/abs/2304.11277"><b>PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</b></a><br>
              </li>
              <li>
                Zhu, et al., 2023, <a href="https://arxiv.org/abs/2304.10592"><b>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</b></a><br>
              </li>              
              <li>
                Chen, et al., 2023, <a href="https://arxiv.org/abs/2310.09478"><b>MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning</b></a><br>
              </li>
              <li>
                Hong, et al., 2023, <a href="https://arxiv.org/abs/2312.08914"><b>CogAgent: A Visual Language Model for GUI Agents</b></a><br>
              </li>
              <li>
                Chen, et al., 2024, <a href="https://arxiv.org/abs/2404.16821"><b>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites</b></a><br>
              </li>
              <li>
                Dehghani, et al., 2023, <a href="https://arxiv.org/abs/2307.06304"><b>Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2305.01278"><b>VPGTrans: Transfer Visual Prompt Generator across LLMs</b></a><br>
              </li>
              <li>
                Wu, et al., 2023, <a href="https://arxiv.org/abs/2309.05519"><b>NExT-GPT: Any-to-Any Multimodal LLM</b></a><br>
              </li>
              <li>
                Fei, et al., 2024, <a href="http://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf"><b>VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing</b></a><br>
              </li>
              <li>
                Zhang, et al., 2024, <a href="https://arxiv.org/abs/2311.04498"><b>NExT-Chat: An LMM for Chat, Detection and Segmentation</b></a><br>
              </li>
            </ol>
        </div>




      </div>
    </section><!-- End Literature Section -->


<!-- ======= Citation Section ======= -->
    <section id="about" class="about">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>Citation</h2>
          <p>Citation</p>
        </div>

          <div style="width=95%">
            <p  style="text-align:justify">
            @inproceedings{fei2024multimodal,<br>
  		title={From Multimodal LLM to Human-level AI: Modality, Instruction, Reasoning, Efficiency and Beyond},<br>
  		author={Fei, Hao and Yao, Yuan and Zhang, Zhuosheng and Liu, Fuxiao and Zhang, Ao and Chua, Tat-Seng},<br>
  		booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024): Tutorial Summaries},<br>
  		pages={1--8},<br>
  		year={2024} <br>
		} <br>
            </p>

        </div>


      </div>
    </section>
    <br>
    <!-- End Citation Section -->




    <!-- ======= Contact Section ======= -->
    <section id="contact" class="contact section-bg">
      <div class="container">

        <div class="section-title" data-aos="zoom-out">
            <h2>Contact</h2>
	    <p>Contact us</p>
	</div>

        <div>
          <h5>Join and post at our <a href="https://groups.google.com/g/mllm24">Google Group</a>!</h5>
          <h5>Email the organziers at <a href="mailto:mllm24@googlegroups.com">mllm24@googlegroups.com</a> .</h5>

        </div>

     </div>

    </section><!-- End Contact Section -->




    <!-- ======= Cite Section ======= -->
<!--    <section id="cite" class="faq section-bg">-->
<!--      <div class="container">-->

<!--        <div class="section-title" data-aos="fade-up">-->
<!--          <h2>Cite</h2>-->
<!--          <p>BibTeX</p>-->
<!--        </div>-->

<!--      <div class="container is-max-desktop content">-->

<!--            <pre><code>@article{ retrieval-lm-tutorial,-->
<!--              author    = { Asai, Akari and Min, Sewon and Zhong, Zexuan and Chen, Danqi },-->
<!--              title     = { ACL 2023 Tutorial: Retrieval-based Language Models and Applications },-->
<!--              journal   = { ACL 2023 },-->
<!--              year      = { 2023 },-->
<!--            }</code></pre>-->
<!--        </div>-->


<!--        <div class="faq-list">-->
<!--          <ul>-->
<!--            <li data-aos="fade-up">-->
<!--              <i class="bx bx-help-circle icon-help"></i> <a data-bs-toggle="collapse" class="collapse" data-bs-target="#faq-list-1">Non consectetur a erat nam at lectus urna duis? <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>-->
<!--              <div id="faq-list-1" class="collapse show" data-bs-parent=".faq-list">-->
<!--                <p>-->
<!--                  Feugiat pretium nibh ipsum consequat. Tempus iaculis urna id volutpat lacus laoreet non curabitur gravida. Venenatis lectus magna fringilla urna porttitor rhoncus dolor purus non.-->
<!--                </p>-->
<!--              </div>-->
<!--            </li>-->

<!--            <li data-aos="fade-up" data-aos-delay="100">-->
<!--              <i class="bx bx-help-circle icon-help"></i> <a data-bs-toggle="collapse" data-bs-target="#faq-list-2" class="collapsed">Feugiat scelerisque varius morbi enim nunc? <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>-->
<!--              <div id="faq-list-2" class="collapse" data-bs-parent=".faq-list">-->
<!--                <p>-->
<!--                  Dolor sit amet consectetur adipiscing elit pellentesque habitant morbi. Id interdum velit laoreet id donec ultrices. Fringilla phasellus faucibus scelerisque eleifend donec pretium. Est pellentesque elit ullamcorper dignissim. Mauris ultrices eros in cursus turpis massa tincidunt dui.-->
<!--                </p>-->
<!--              </div>-->
<!--            </li>-->

<!--            <li data-aos="fade-up" data-aos-delay="200">-->
<!--              <i class="bx bx-help-circle icon-help"></i> <a data-bs-toggle="collapse" data-bs-target="#faq-list-3" class="collapsed">Dolor sit amet consectetur adipiscing elit? <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>-->
<!--              <div id="faq-list-3" class="collapse" data-bs-parent=".faq-list">-->
<!--                <p>-->
<!--                  Eleifend mi in nulla posuere sollicitudin aliquam ultrices sagittis orci. Faucibus pulvinar elementum integer enim. Sem nulla pharetra diam sit amet nisl suscipit. Rutrum tellus pellentesque eu tincidunt. Lectus urna duis convallis convallis tellus. Urna molestie at elementum eu facilisis sed odio morbi quis-->
<!--                </p>-->
<!--              </div>-->
<!--            </li>-->

<!--            <li data-aos="fade-up" data-aos-delay="300">-->
<!--              <i class="bx bx-help-circle icon-help"></i> <a data-bs-toggle="collapse" data-bs-target="#faq-list-4" class="collapsed">Tempus quam pellentesque nec nam aliquam sem et tortor consequat? <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>-->
<!--              <div id="faq-list-4" class="collapse" data-bs-parent=".faq-list">-->
<!--                <p>-->
<!--                  Molestie a iaculis at erat pellentesque adipiscing commodo. Dignissim suspendisse in est ante in. Nunc vel risus commodo viverra maecenas accumsan. Sit amet nisl suscipit adipiscing bibendum est. Purus gravida quis blandit turpis cursus in.-->
<!--                </p>-->
<!--              </div>-->
<!--            </li>-->

<!--            <li data-aos="fade-up" data-aos-delay="400">-->
<!--              <i class="bx bx-help-circle icon-help"></i> <a data-bs-toggle="collapse" data-bs-target="#faq-list-5" class="collapsed">Tortor vitae purus faucibus ornare. Varius vel pharetra vel turpis nunc eget lorem dolor? <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>-->
<!--              <div id="faq-list-5" class="collapse" data-bs-parent=".faq-list">-->
<!--                <p>-->
<!--                  Laoreet sit amet cursus sit amet dictum sit amet justo. Mauris vitae ultricies leo integer malesuada nunc vel. Tincidunt eget nullam non nisi est sit amet. Turpis nunc eget lorem dolor sed. Ut venenatis tellus in metus vulputate eu scelerisque.-->
<!--                </p>-->
<!--              </div>-->
<!--            </li>-->

<!--          </ul>-->
<!--        </div>-->

<!--      </div>-->
<!--    </section>-->
    <!-- End F.A.Q Section -->





    <!-- ======= Contact Section ======= -->
<!--    <section id="contact" class="contact">-->
<!--      <div class="container">-->

<!--        <div class="section-title" data-aos="fade-up">-->
<!--          <h2>Contact</h2>-->
<!--          <p>Contact Us</p>-->
<!--        </div>-->

<!--        <div class="row">-->

<!--          <div class="col-lg-4" data-aos="fade-right" data-aos-delay="100">-->
<!--            <div class="info">-->
<!--              <div class="address">-->
<!--                <i class="bi bi-geo-alt"></i>-->
<!--                <h4>Location:</h4>-->
<!--                <p>A108 Adam Street, New York, NY 535022</p>-->
<!--              </div>-->

<!--              <div class="email">-->
<!--                <i class="bi bi-envelope"></i>-->
<!--                <h4>Email:</h4>-->
<!--                <p>info@example.com</p>-->
<!--              </div>-->

<!--              <div class="phone">-->
<!--                <i class="bi bi-phone"></i>-->
<!--                <h4>Call:</h4>-->
<!--                <p>+1 5589 55488 55s</p>-->
<!--              </div>-->

<!--            </div>-->

<!--          </div>-->

<!--          <div class="col-lg-8 mt-5 mt-lg-0" data-aos="fade-left" data-aos-delay="200">-->

<!--            <form action="forms/contact.php" method="post" role="form" class="php-email-form">-->
<!--              <div class="row">-->
<!--                <div class="col-md-6 form-group">-->
<!--                  <input type="text" name="name" class="form-control" id="name" placeholder="Your Name" required>-->
<!--                </div>-->
<!--                <div class="col-md-6 form-group mt-3 mt-md-0">-->
<!--                  <input type="email" class="form-control" name="email" id="email" placeholder="Your Email" required>-->
<!--                </div>-->
<!--              </div>-->
<!--              <div class="form-group mt-3">-->
<!--                <input type="text" class="form-control" name="subject" id="subject" placeholder="Subject" required>-->
<!--              </div>-->
<!--              <div class="form-group mt-3">-->
<!--                <textarea class="form-control" name="message" rows="5" placeholder="Message" required></textarea>-->
<!--              </div>-->
<!--              <div class="my-3">-->
<!--                <div class="loading">Loading</div>-->
<!--                <div class="error-message"></div>-->
<!--                <div class="sent-message">Your message has been sent. Thank you!</div>-->
<!--              </div>-->
<!--              <div class="text-center"><button type="submit">Send Message</button></div>-->
<!--            </form>-->

<!--          </div>-->

<!--        </div>-->

<!--      </div>-->
<!--    </section>&lt;!&ndash; End Contact Section &ndash;&gt;-->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
<!--    <div class="footer-top">-->
<!--      <div class="container">-->
<!--        <div class="row">-->

<!--          <div class="col-lg-4 col-md-6">-->
<!--            <div class="footer-info">-->
<!--              <h3>Bootslander</h3>-->
<!--              <p class="pb-3"><em>Qui repudiandae et eum dolores alias sed ea. Qui suscipit veniam excepturi quod.</em></p>-->
<!--              <p>-->
<!--                A108 Adam Street <br>-->
<!--                NY 535022, USA<br><br>-->
<!--                <strong>Phone:</strong> +1 5589 55488 55<br>-->
<!--                <strong>Email:</strong> info@example.com<br>-->
<!--              </p>-->
<!--              <div class="social-links mt-3">-->
<!--                <a href="#" class="twitter"><i class="bx bxl-twitter"></i></a>-->
<!--                <a href="#" class="facebook"><i class="bx bxl-facebook"></i></a>-->
<!--                <a href="#" class="instagram"><i class="bx bxl-instagram"></i></a>-->
<!--                <a href="#" class="google-plus"><i class="bx bxl-skype"></i></a>-->
<!--                <a href="#" class="linkedin"><i class="bx bxl-linkedin"></i></a>-->
<!--              </div>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-2 col-md-6 footer-links">-->
<!--            <h4>Useful Links</h4>-->
<!--            <ul>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Home</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">About us</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Services</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Terms of service</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Privacy policy</a></li>-->
<!--            </ul>-->
<!--          </div>-->

<!--          <div class="col-lg-2 col-md-6 footer-links">-->
<!--            <h4>Our Services</h4>-->
<!--            <ul>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Web Design</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Web Development</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Product Management</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Marketing</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Graphic Design</a></li>-->
<!--            </ul>-->
<!--          </div>-->

<!--          <div class="col-lg-4 col-md-6 footer-newsletter">-->
<!--            <h4>Our Newsletter</h4>-->
<!--            <p>Tamen quem nulla quae legam multos aute sint culpa legam noster magna</p>-->
<!--            <form action="" method="post">-->
<!--              <input type="email" name="email"><input type="submit" value="Subscribe">-->
<!--            </form>-->

<!--          </div>-->

<!--        </div>-->
<!--      </div>-->
<!--    </div>-->

    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong><span>Bootslander</span></strong>. All Rights Reserved
      </div>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/bootslander-free-bootstrap-landing-page-template/ -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>
  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
